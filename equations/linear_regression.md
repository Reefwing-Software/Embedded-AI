<!--
 Copyright (c) 2024 David Such
 
 This software is released under the MIT License.
 https://opensource.org/licenses/MIT
-->

<!-- Linear Regression hypothesis -->
$$
Y = \beta_0 + \beta_1 X
$$

<br>

<!-- Mean Squared Error (MSE) Cost Function in Linear Regression -->
$$
J(\beta) = \frac{1}{m} \sum_{i=1}^{m} \left( h_{\beta}(x^{(i)}) - y^{(i)} \right)^2 
$$

$$
\begin{flushleft}
\textbf{Where:} \\
$m$ is the number of observations (data points), \\
$h_{\beta}(x^{(i)})$ is the predicted value of $y$ for the $i$-th data point, \\
$y^{(i)}$ is the actual observed value of $y$ for the $i$-th data point, \\
$\beta$ represents the coefficients (parameters) of the linear regression model.
\end{flushleft}
$$

<br>

\[
\textbf{Where: } 
\]

\[
m \text{ is the number of observations (data points)},
\]

\[
h_{\beta}(x^{(i)}) \text{ is the predicted value of } y \text{ for the } i\text{-th data point}, 
\]

\[
y^{(i)} \text{ is the actual observed value of } y \text{ for the } i\text{-th data point},
\]

\[
\beta \text{ represents the coefficients (parameters) of the linear regression model}.
\]

<br>
